{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f89ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import math\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84154d",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c9ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.linspace(-1,1,256)\n",
    "x_2 = np.linspace(1,-1,256)\n",
    "x_3 = np.linspace(-1,1,256)\n",
    "\n",
    "X, Y,Z = np.meshgrid(x_1,x_2,x_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d4f6e",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "We prepare the test data to compare against the solution produced by the WAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99560bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v_test = np.hstack((X.flatten(order='F')[:,None], Y.flatten(order='F')[:,None], Z.flatten(order='F')[:,None]))\n",
    "\n",
    "lb = np.array([-1, -1, -1]) #lower bound\n",
    "ub = np.array([1, 1, 1])  #upper bound\n",
    "\n",
    "vsol = 1/(4 * np.pi*np.sqrt(X**2 + Y**2+Z**2))+np.sin(X*Y+Z)\n",
    "\n",
    "v_true = vsol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05d479",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a92c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_v,N_f):\n",
    "    \n",
    "    leftedge_x = np.hstack((X[0,:].flatten('a')[:,None], Y[0,:].flatten('a')[:,None], Z[0,:].flatten('a')[:,None]))\n",
    "    leftedge_v = vsol[0,:].flatten('a')[:,None]\n",
    "    \n",
    "    rightedge_x = np.hstack((X[-1,:].flatten('a')[:,None], Y[-1,:].flatten('a')[:,None], Z[-1,:].flatten('a')[:,None]))\n",
    "    rightedge_v = vsol[-1,:].flatten('a')[:,None]\n",
    "    \n",
    "    backedge_x = np.hstack((X[:,0].flatten('a')[:,None], Y[:,0].flatten('a')[:,None], Z[:,0].flatten('a')[:,None]))\n",
    "    backedge_v = vsol[:,0].flatten('a')[:,None]\n",
    "    \n",
    "    frontedge_x = np.hstack((X[:,-1].flatten('a')[:,None], Y[:,-1].flatten('a')[:,None], Z[:,-1].flatten('a')[:,None]))\n",
    "    frontedge_v = vsol[:,-1].flatten('a')[:,None]\n",
    "    \n",
    "    bottomedge_x = np.hstack((X[:,:,0].flatten('a')[:,None], Y[:,:,0].flatten('a')[:,None], Z[:,:,0].flatten('a')[:,None]))\n",
    "    bottomedge_v = vsol[:,:,0].flatten('a')[:,None]\n",
    "    \n",
    "    topedge_x = np.hstack((X[:,:,-1].flatten('a')[:,None], Y[:,:,-1].flatten('a')[:,None], Z[:,:,-1].flatten('a')[:,None]))\n",
    "    topedge_v = vsol[:,:,-1].flatten('a')[:,None]\n",
    "    \n",
    "    all_X_v_train = np.vstack([leftedge_x, rightedge_x, backedge_x, frontedge_x, bottomedge_x, topedge_x])\n",
    "    all_v_train = np.vstack([leftedge_v, rightedge_v, backedge_v, frontedge_v, bottomedge_v, topedge_v])\n",
    "     \n",
    "    #choose random N_v points for training\n",
    "    idx = np.random.choice(all_X_v_train.shape[0], N_v, replace=False) \n",
    "    \n",
    "    X_v_train = all_X_v_train[idx[0:N_v], :] #choose indices from  set 'idx' (x,t)\n",
    "    v_train = all_v_train[idx[0:N_v],:]      #choose corresponding v\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # N_f sets of tuples(x,t)\n",
    "    X_f = lb + (ub-lb)*lhs(3,N_f)\n",
    "    \n",
    "    return X_f, X_v_train, v_train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf6b7f",
   "metadata": {},
   "source": [
    "# WAN\n",
    "\n",
    "Creating sequential layers using the class\n",
    "tf.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f82fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel0(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers0):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "    \n",
    "        'Initialise neural network as a nn.MSELosslist using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers0[i], layers0[i+1]) for i in range(len(layers0)-1)])\n",
    "        \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers0)-1):\n",
    "            \n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "                        \n",
    "        for i in range(len(layers0)-2):\n",
    "            \n",
    "            z = self.linears[i](a)\n",
    "                        \n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def loss(self,x_to_train_f):\n",
    "\n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "        x_3_f = x_to_train_f[:,[2]]\n",
    "                        \n",
    "        g = x_to_train_f.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        uu = self.forward(g)\n",
    "\n",
    "        uu_x = autograd.grad(uu,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                                            \n",
    "        uu_x_1 = uu_x[:,[0]]\n",
    "        \n",
    "        uu_x_2 = uu_x[:,[1]]\n",
    "        \n",
    "        uu_x_3 = uu_x[:,[2]]\n",
    "                        \n",
    "        k = x_1_f**2 + x_2_f**2 + x_3_f**2 + 1\n",
    "\n",
    "        gg = (x_1_f**2 + x_2_f**2 + x_3_f**2 + 1)*(x_1_f**2 + x_2_f**2 +1)*np.sin(x_1_f*x_2_f+x_3_f)-(4*x_1_f*x_2_f+2*x_3_f)*np.cos(x_1_f*x_2_f+x_3_f)+1/(2*np.pi*np.sqrt(x_1_f**2 + x_2_f**2 + x_3_f**2))\n",
    "        f = torch.log(torch.square(torch.mean(k * (uu_x_1*v_x_1 + uu_x_2*v_x_2+ uu_x_3*v_x_3))- 1e5-torch.mean(gg*uu)))-torch.log(torch.mean(torch.square(uu)))\n",
    "        \n",
    "        f = -f\n",
    "\n",
    "        loss = torch.mean(f)\n",
    "\n",
    "        return loss\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_val = self.loss(X_f_train)\n",
    "        \n",
    "        #error_vec, _ = WAN.test()\n",
    "        \n",
    "        #print(loss_val,error_vec)\n",
    "        \n",
    "        loss_val.backward(retain_graph=True)\n",
    "\n",
    "        return loss_val        \n",
    "    \n",
    "    def test(self):\n",
    "                \n",
    "        v_pred = self.forward(X_v_test_tensor)\n",
    "        \n",
    "        error_vec = torch.linalg.norm((vt-v_pred),2)/torch.linalg.norm(vt,2)        # Relative L2 Norm of the error (Vector)\n",
    "        \n",
    "        v_pred = np.reshape(v_pred.cpu().detach().numpy(),(256,256,256),order='F') \n",
    "        \n",
    "        return error_vec, v_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034cbed",
   "metadata": {},
   "source": [
    "# WAN0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d31935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "    \n",
    "        'Initialise neural network as a nn.MSELosslist using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = self.linears[i](a)\n",
    "                        \n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a)\n",
    "        \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,x,y):\n",
    "                \n",
    "        loss_v = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_v\n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f):\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "        x_3_f = x_to_train_f[:,[2]]\n",
    "        \n",
    "        g = x_to_train_f.clone()\n",
    "                        \n",
    "        g.requires_grad_(True)\n",
    "        \n",
    "        vv = self.forward(g)\n",
    "                \n",
    "        vv_x = autograd.grad(vv,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                                            \n",
    "        vv_x_1 = vv_x[:,[0]]\n",
    "        \n",
    "        vv_x_2 = vv_x[:,[1]]\n",
    "        \n",
    "        vv_x_3 = vv_x[:,[2]]\n",
    "                        \n",
    "        k = x_1_f**2 + x_2_f**2 + x_3_f**2 + 1\n",
    "\n",
    "        gg = (x_1_f**2 + x_2_f**2 + x_3_f**2 + 1)*(x_1_f**2 + x_2_f**2 +1)*np.sin(x_1_f*x_2_f+x_3_f)-(4*x_1_f*x_2_f+2*x_3_f)*np.cos(x_1_f*x_2_f+x_3_f)+1/(2*np.pi*np.sqrt(x_1_f**2 + x_2_f**2 + x_3_f**2))\n",
    "        f = torch.log(torch.square(torch.mean(k * (u_x_1*vv_x_1 + u_x_2*vv_x_2+ u_x_3*vv_x_3))- 1e5-torch.mean(gg*u)))-torch.log(torch.mean(torch.square(u)))\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def loss(self,x,y,x_to_train_f):\n",
    "\n",
    "        loss_v = self.loss_BC(x,y)\n",
    "        loss_f = self.loss_PDE(x_to_train_f)\n",
    "\n",
    "        loss = 1153/2 * loss_v + loss_f\n",
    "\n",
    "        return loss\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_val = self.loss(X_v_train, v_train, X_f_train)\n",
    "        \n",
    "        #error_vec, _ = WAN0.test()\n",
    "        \n",
    "        #print(loss_val,error_vec)\n",
    "        \n",
    "        global ite, err_vec, start_time\n",
    "        ite = ite + 1\n",
    "        \n",
    "        if (ite % 100 == 0):\n",
    "            time_vec.append(time.time() - start_time)\n",
    "            error_vec, _ = WAN0.test()\n",
    "            err_vec.append(error_vec.detach().float().item())\n",
    "        \n",
    "        loss_val.backward(retain_graph=True)\n",
    "\n",
    "        return loss_val        \n",
    "    \n",
    "    def test(self):\n",
    "                \n",
    "        v_pred = self.forward(X_v_test_tensor)\n",
    "        \n",
    "        error_vec = torch.linalg.norm((vt-v_pred),2)/torch.linalg.norm(vt,2)        # Relative L2 Norm of the error (Vector)\n",
    "        \n",
    "        v_pred = np.reshape(v_pred.cpu().detach().numpy(),(256,256,256),order='F') \n",
    "        \n",
    "        return error_vec, v_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9853d",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71f69f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 57.02\n",
      "Test Error: 0.06563\n",
      "[4.189789295196533, 10.732282876968384, 14.179094076156616, 17.638709545135498, 20.74244499206543, 23.621702671051025, 26.552860021591187, 29.290534496307373, 32.04020643234253, 34.91648030281067, 37.92842173576355, 40.790764808654785, 43.6441285610199, 46.36587834358215, 49.09358215332031, 51.73647713661194, 54.378440856933594]\n",
      "[0.4417053163051605, 0.3198179304599762, 0.18629775941371918, 0.19712841510772705, 0.1873161941766739, 0.1933523714542389, 0.199367493391037, 0.2032577097415924, 0.19613927602767944, 0.16525226831436157, 0.16073870658874512, 0.16797934472560883, 0.16112980246543884, 0.11776256561279297, 0.07108234614133835, 0.06725131720304489, 0.06520531326532364]\n"
     ]
    }
   ],
   "source": [
    "N_v = 600 \n",
    "N_f = 10000 \n",
    "\n",
    "X_f_train_np_array, X_v_train_np_array, v_train_np_array = trainingdata(N_v,N_f)\n",
    "\n",
    "'Convert to tensor and send to GPU'\n",
    "X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "X_v_train = torch.from_numpy(X_v_train_np_array).float().to(device)\n",
    "v_train = torch.from_numpy(v_train_np_array).float().to(device)\n",
    "X_v_test_tensor = torch.from_numpy(X_v_test).float().to(device)\n",
    "vt = torch.from_numpy(v_true).float().to(device)\n",
    "\n",
    "layers = np.array([3, 5, 5, 1])\n",
    "layers0 = np.array([3, 5, 5, 1])\n",
    "\n",
    "WAN0 = Sequentialmodel(layers)\n",
    "WAN0.to(device)\n",
    "\n",
    "time_vec = []\n",
    "err_vec = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "x_1_f = X_f_train[:,[0]]\n",
    "x_2_f = X_f_train[:,[1]]\n",
    "x_3_f = X_f_train[:,[2]]\n",
    "g = X_f_train.clone()                       \n",
    "g.requires_grad = True      \n",
    "v = WAN0.forward(g)              \n",
    "v_x = autograd.grad(v,g,torch.ones([X_f_train.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "v_x_1 = v_x[:,[0]]\n",
    "v_x_2 = v_x[:,[1]]\n",
    "v_x_3 = v_x[:,[2]]\n",
    "\n",
    "WAN = Sequentialmodel0(layers0)\n",
    "WAN.to(device)\n",
    "\n",
    "optimizer = torch.optim.LBFGS(WAN.parameters(), lr=0.11, \n",
    "                              max_iter = 1000, \n",
    "                              max_eval = 2500, \n",
    "                              tolerance_grad = 1e-06, \n",
    "                              tolerance_change = 1e-09, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "optimizer.step(WAN.closure)\n",
    "\n",
    "x_1_f = X_f_train[:,[0]]\n",
    "x_2_f = X_f_train[:,[1]]\n",
    "x_3_f = X_f_train[:,[2]]\n",
    "g = X_f_train.clone()                       \n",
    "g.requires_grad = True      \n",
    "u = WAN.forward(g)              \n",
    "u_x = autograd.grad(u,g,torch.ones([X_f_train.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]                                                          \n",
    "u_x_1 = u_x[:,[0]]      \n",
    "u_x_2 = u_x[:,[1]]\n",
    "u_x_3 = u_x[:,[2]]\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "\n",
    "ite = 0\n",
    "optimizer = torch.optim.LBFGS(WAN0.parameters(), lr=0.11, \n",
    "                              max_iter = 5000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = 1e-06, \n",
    "                              tolerance_change = 1e-09, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "optimizer.step(WAN0.closure)\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "\n",
    "''' Model Accuracy ''' \n",
    "error_vec, v_pred = WAN0.test()\n",
    "\n",
    "print('Test Error: %.5f'  % (error_vec))\n",
    "print(time_vec)\n",
    "print(err_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
