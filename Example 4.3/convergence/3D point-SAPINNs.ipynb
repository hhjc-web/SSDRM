{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dd392b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "#hide tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'},\n",
    "#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import codecs, json\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d5398",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66118740",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.linspace(-1,1,256)\n",
    "x_2 = np.linspace(1,-1,256)\n",
    "x_3 = np.linspace(-1,1,256)\n",
    "\n",
    "X, Y, Z = np.meshgrid(x_1,x_2,x_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91980015",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "We prepare the test data to compare against the solution produced by the SAPINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec6dc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v_test = np.hstack((X.flatten(order='F')[:,None], Y.flatten(order='F')[:,None],Z.flatten(order='F')[:,None]))\n",
    "\n",
    "lb = np.array([-1, -1, -1]) #lower bound\n",
    "ub = np.array([1, 1, 1])  #upper bound\n",
    "\n",
    "vsol = 1/(4 * np.pi*np.sqrt(X**2 + Y**2+Z**2))+np.sin(X*Y+Z)\n",
    "\n",
    "v = vsol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21697f",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "756de928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_v,N_f):\n",
    "    \n",
    "    leftedge_x = np.hstack((X[0,:].flatten('a')[:,None], Y[0,:].flatten('a')[:,None], Z[0,:].flatten('a')[:,None]))\n",
    "    leftedge_v = vsol[0,:].flatten('a')[:,None]\n",
    "    \n",
    "    rightedge_x = np.hstack((X[-1,:].flatten('a')[:,None], Y[-1,:].flatten('a')[:,None], Z[-1,:].flatten('a')[:,None]))\n",
    "    rightedge_v = vsol[-1,:].flatten('a')[:,None]\n",
    "    \n",
    "    backedge_x = np.hstack((X[:,0].flatten('a')[:,None], Y[:,0].flatten('a')[:,None], Z[:,0].flatten('a')[:,None]))\n",
    "    backedge_v = vsol[:,0].flatten('a')[:,None]\n",
    "    \n",
    "    frontedge_x = np.hstack((X[:,-1].flatten('a')[:,None], Y[:,-1].flatten('a')[:,None], Z[:,-1].flatten('a')[:,None]))\n",
    "    frontedge_v = vsol[:,-1].flatten('a')[:,None]\n",
    "    \n",
    "    bottomedge_x = np.hstack((X[:,:,0].flatten('a')[:,None], Y[:,:,0].flatten('a')[:,None], Z[:,:,0].flatten('a')[:,None]))\n",
    "    bottomedge_v = vsol[:,:,0].flatten('a')[:,None]\n",
    "    \n",
    "    topedge_x = np.hstack((X[:,:,-1].flatten('a')[:,None], Y[:,:,-1].flatten('a')[:,None], Z[:,:,-1].flatten('a')[:,None]))\n",
    "    topedge_v = vsol[:,:,-1].flatten('a')[:,None]\n",
    "    \n",
    "    all_X_v_train = np.vstack([leftedge_x, rightedge_x, backedge_x, frontedge_x, bottomedge_x, topedge_x])\n",
    "    all_v_train = np.vstack([leftedge_v, rightedge_v, backedge_v, frontedge_v, bottomedge_v, topedge_v])  \n",
    "     \n",
    "    #choose random N_v points for training\n",
    "    idx = np.random.choice(all_X_v_train.shape[0], N_v, replace=False) \n",
    "    \n",
    "    X_v_train = all_X_v_train[idx[0:N_v], :] #choose indices from  set 'idx' (x,t)\n",
    "    v_train = all_v_train[idx[0:N_v],:]      #choose corresponding v\n",
    "    \n",
    "    #Collocation Points\n",
    "\n",
    "    X_f = lb + (ub-lb)*lhs(3,N_f)\n",
    "    X_f_train = np.vstack((X_f, X_v_train)) # append training points to collocation points \n",
    "    \n",
    "    return X_f_train, X_v_train, v_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e50650",
   "metadata": {},
   "source": [
    "# PINN\n",
    "\n",
    "Creating sequential layers using the class tf.Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "690a1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(tf.Module): \n",
    "    def __init__(self, layers, name=None):\n",
    "\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0 #total number of parameters\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "\n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i+1]\n",
    "\n",
    "            #Xavier standard deviation \n",
    "            std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
    "\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype = 'float64') * std_dv\n",
    "\n",
    "            w = tf.Variable(w, trainable=True, name = 'w' + str(i+1))\n",
    "\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype = 'float64'), trainable = True, name = 'b' + str(i+1))\n",
    "\n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "\n",
    "            self.parameters +=  input_dim * output_dim + output_dim\n",
    "            \n",
    "        # Lagrange multipliers\n",
    "        \n",
    "        # Boundary terms      \n",
    "        self.lagrange_1 = tf.Variable(tf.cast(tf.ones([N_v,1]), dtype = 'float64'), trainable = True) \n",
    "        \n",
    "        # Residual terms\n",
    "        self.lagrange_2 = tf.Variable(tf.cast(tf.ones([N_f+N_v,1]), dtype = 'float64'), trainable = True)\n",
    "        \n",
    "        # Circle terms\n",
    "        self.lagrange_3 = tf.Variable(tf.cast(tf.ones([N_f+N_v,1]), dtype = 'float64'), trainable = True) \n",
    "\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        \n",
    "        #preprocessing input \n",
    "        x = (x - lb)/(ub - lb) #feature scaling\n",
    "        \n",
    "        a = x\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = tf.add(tf.matmul(a, self.W[2*i]), self.W[2*i+1])\n",
    "            a = tf.nn.tanh(z)\n",
    "            \n",
    "        a = tf.add(tf.matmul(a, self.W[-2]), self.W[-1]) # For regression, no activation to last layer\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def get_weights(self):\n",
    "\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "            \n",
    "            w_1d = tf.reshape(self.W[2*i],[-1])   #flatten weights \n",
    "            b_1d = tf.reshape(self.W[2*i+1],[-1]) #flatten biases\n",
    "            \n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0) #concat weights \n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0) #concat biases\n",
    "        \n",
    "        return parameters_1d\n",
    "        \n",
    "    def set_weights(self,parameters):\n",
    "                \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            shape_w = tf.shape(self.W[2*i]).numpy() # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2*i]).numpy() #size of the weight tensor \n",
    "            \n",
    "            shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2*i+1]).numpy() #size of the bias tensor \n",
    "                        \n",
    "            pick_w = parameters[0:size_w] #pick the weights \n",
    "            self.W[2*i].assign(tf.reshape(pick_w,shape_w)) # assign  \n",
    "            parameters = np.delete(parameters,np.arange(size_w),0) #delete \n",
    "            \n",
    "            pick_b = parameters[0:size_b] #pick the biases \n",
    "            self.W[2*i+1].assign(tf.reshape(pick_b,shape_b)) # assign \n",
    "            parameters = np.delete(parameters,np.arange(size_b),0) #delete \n",
    "\n",
    "            \n",
    "    def loss_BC(self,x,y):\n",
    "\n",
    "        g = tf.Variable(x, dtype = 'float64', trainable = False)\n",
    "        \n",
    "        x_1_f = g[:,0:1]\n",
    "        x_2_f = g[:,1:2]\n",
    "        x_3_f = g[:,2:3]\n",
    "        \n",
    "        s = y-self.evaluate(x)\n",
    "        s = self.lagrange_1*s\n",
    "        loss_v = tf.reduce_mean(tf.square(s))\n",
    "        \n",
    "        return loss_v\n",
    "\n",
    "    def loss_PDE(self, x_to_train_f):\n",
    "    \n",
    "        g = tf.Variable(x_to_train_f, dtype = 'float64', trainable = False)\n",
    "        \n",
    "        x_1_f = g[:,0:1]\n",
    "        x_2_f = g[:,1:2]\n",
    "        x_3_f = g[:,2:3]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            tape.watch(x_1_f)\n",
    "            tape.watch(x_2_f)\n",
    "            tape.watch(x_3_f)\n",
    "\n",
    "            g = tf.stack([x_1_f[:,0], x_2_f[:,0], x_3_f[:,0]], axis=1)\n",
    "\n",
    "            v = self.evaluate(g)\n",
    "            v_x_1 = tape.gradient(v,x_1_f)\n",
    "            v_x_2 = tape.gradient(v,x_2_f)\n",
    "            v_x_3 = tape.gradient(v,x_3_f)\n",
    "\n",
    "        v_xx_1 = tape.gradient(v_x_1,x_1_f)\n",
    "        v_xx_2 = tape.gradient(v_x_2,x_2_f)\n",
    "        v_xx_3 = tape.gradient(v_x_3,x_3_f)\n",
    "\n",
    "        del tape\n",
    "        \n",
    "        H = 0.075\n",
    "        \n",
    "        k = x_1_f**2 + x_2_f**2 + x_3_f**2 + 1\n",
    "        q = 12/(np.pi * H**2) * (5/H**2 * (x_1_f**2 + x_2_f**2+ x_3_f**2) - 8/H * np.sqrt(x_1_f**2 + x_2_f**2+ x_3_f**2) + 3)\n",
    "        q = tf.where(x_1_f**2+x_2_f**2+ x_3_f**2 >= H**2, 0.0, q)\n",
    "        \n",
    "        gg = (x_1_f**2 + x_2_f**2 + x_3_f**2 + 1)*(x_1_f**2 + x_2_f**2 +1)*np.sin(x_1_f*x_2_f+x_3_f)-(4*x_1_f*x_2_f+2*x_3_f)*np.cos(x_1_f*x_2_f+x_3_f)+1/(2*np.pi*np.sqrt(x_1_f**2 + x_2_f**2 + x_3_f**2))\n",
    "        \n",
    "        f = -k*(v_xx_1 + v_xx_2 + v_xx_3) - 2*x_1_f*v_x_1 - 2*x_2_f*v_x_2 - 2*x_3_f*v_x_3 - k*q-gg\n",
    "\n",
    "        f = tf.where(x_1_f**2+x_2_f**2+x_3_f**2 >= H**2, self.lagrange_2 * f, f)\n",
    "        f = tf.where(x_1_f**2+x_2_f**2+x_3_f**2 < H**2, self.lagrange_3 * f, f)\n",
    "        \n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "\n",
    "        return loss_f, f\n",
    "    \n",
    "    def loss(self,x,y,g,sigma):\n",
    "\n",
    "        loss_v = self.loss_BC(x,y)\n",
    "        loss_f, f = self.loss_PDE(g)\n",
    "\n",
    "        loss = sigma/2 * loss_v + loss_f\n",
    "\n",
    "        return loss, loss_v, loss_f \n",
    "    \n",
    "    def optimizerfunc(self,parameters):\n",
    "        \n",
    "        self.set_weights(parameters)\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            tape.watch(self.trainable_variables)\n",
    "            loss_val, loss_v, loss_f = self.loss(X_v_train, v_train, X_f_train, sigma)\n",
    "            \n",
    "        grads = tape.gradient(loss_val,self.trainable_variables)\n",
    "        \n",
    "        del tape\n",
    "        \n",
    "        grads_1d = [ ] #store 1d grads \n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            grads_w_1d = tf.reshape(grads[2*i],[-1]) #flatten weights \n",
    "            grads_b_1d = tf.reshape(grads[2*i+1],[-1]) #flatten biases\n",
    "\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0) #concat grad_weights \n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0) #concat grad_biases\n",
    "        \n",
    "        return loss_val.numpy(), grads_1d.numpy()\n",
    "    \n",
    "    def optimizer_callback(self,parameters):\n",
    "                \n",
    "        loss_value, loss_v, loss_f = self.loss(X_v_train, v_train, X_f_train, sigma)\n",
    "        \n",
    "        v_pred = self.evaluate(X_v_test)\n",
    "        error_vec = np.linalg.norm((v-v_pred),2)/np.linalg.norm(v,2)\n",
    "        \n",
    "        tf.print(loss_value, loss_v, loss_f, error_vec)\n",
    "        \n",
    "    def adaptive_gradients(self):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.W)\n",
    "            loss_val, loss_v, loss_f = self.loss(X_v_train, v_train, X_f_train, sigma)\n",
    "\n",
    "        grads = tape.gradient(loss_val,self.W)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        with tf.GradientTape(persistent = True) as tape:\n",
    "            tape.watch(self.lagrange_1)\n",
    "            tape.watch(self.lagrange_2)\n",
    "            tape.watch(self.lagrange_3)\n",
    "            loss_val, loss_v, loss_f = self.loss(X_v_train, v_train, X_f_train,sigma)\n",
    "\n",
    "        grads_L1 = tape.gradient(loss_val,self.lagrange_1) # boundary terms\n",
    "        grads_L2 = tape.gradient(loss_val,self.lagrange_2) # residual terms\n",
    "        grads_L3 = tape.gradient(loss_val,self.lagrange_3)\n",
    "        \n",
    "        del tape\n",
    "\n",
    "        return loss_val, grads, grads_L1, grads_L2, grads_L3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02117d7",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "The loss function consists of two parts:\n",
    "\n",
    "loss_BC: MSE error of boundary losses\n",
    "loss_PDE: MSE error of collocation points satisfying the PDE\n",
    "\n",
    "loss = loss_BC + loss_PDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79612b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.357011818168\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_5276/1632250796.py:46: OptimizeWarning: Unknown solver options: maxcor, ftol, maxfun, iprint, maxls\n",
      "  results = scipy.optimize.minimize(fun = PINN.optimizerfunc,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52383826453731053\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_5276/1632250796.py:92: OptimizeWarning: Unknown solver options: maxcor, ftol, maxfun, iprint, maxls\n",
      "  results = scipy.optimize.minimize(fun = PINN.optimizerfunc,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12416519166930695\n",
      "0.04843077684901731\n",
      "0.029653338770872402\n",
      "0.02242965656926859\n",
      "0.01785200243341974\n",
      "0.015815514657602878\n",
      "0.015019332376539442\n",
      "0.014594031848754848\n",
      "0.014275726145896867\n",
      "0.01396651899406743\n",
      "0.01376088876626501\n",
      "0.013538001276226716\n",
      "0.013323449823427714\n",
      "0.013094968774045746\n",
      "0.01282745768824604\n",
      "0.012603863894164779\n",
      "0.012332049856907642\n",
      "0.012143460076640057\n",
      "0.011997219490094857\n",
      "Training time: 381.91\n",
      "[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]\n",
      "[0.10824996866732187, 0.10035482579050334, 0.09553931699953519, 0.09520325791635274, 0.09376741791025317, 0.09379267983027323, 0.09352553368986054, 0.0935240242033914, 0.09358932430787678, 0.09401957041257544, 0.0935762863376287, 0.09353384618674976, 0.09356552505002386, 0.09364326440397155, 0.09343868622675133, 0.09324676069575197, 0.09297196074197739, 0.09280255374541165, 0.09286439888029127, 0.09267334068270981, 0.09273274541014127]\n"
     ]
    }
   ],
   "source": [
    "N_v = 600 #Total number of data points for 'u'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "# Training data\n",
    "X_f_train, X_v_train, v_train = trainingdata(N_v,N_f)\n",
    "\n",
    "iter_vec = []\n",
    "error_vec = []\n",
    "\n",
    "layers = np.array([3,5,5,1]) #2 hidden layers\n",
    "\n",
    "maxcor = 200 \n",
    "max_iter = 1\n",
    "iteration = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sigma = 1153.3\n",
    "\n",
    "PINN = Sequentialmodel(layers,sigma)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "optimizer_L1 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "optimizer_L2 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "optimizer_L3 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "        loss_value, grads, grads_L1, grads_L2, grads_L3 = PINN.adaptive_gradients()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            tf.print(loss_value)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, PINN.W)) #gradient descent weights \n",
    "        optimizer_L1.apply_gradients(zip([-grads_L1], [PINN.lagrange_1])) # gradient ascent adaptive coefficients of boundary residual\n",
    "        optimizer_L2.apply_gradients(zip([-grads_L2], [PINN.lagrange_2])) # gradient ascent adaptive coefficients of PDE residual\n",
    "        optimizer_L3.apply_gradients(zip([-grads_L3], [PINN.lagrange_3]))  \n",
    "\n",
    "init_params = PINN.get_weights().numpy()\n",
    "\n",
    "results = scipy.optimize.minimize(fun = PINN.optimizerfunc,\n",
    "                                      x0 = init_params,\n",
    "                                      args=(),\n",
    "                                      method='BFGS',\n",
    "                                      jac= True, # If jac is True, fun is assumed to return the gradient along with the objective function\n",
    "                                      callback = None, \n",
    "                                      options = {'disp': None,\n",
    "                                                'maxcor': maxcor, \n",
    "                                                'ftol': 5e-10, # 1 * np.finfo(float).eps,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                                'gtol': 5e-10, \n",
    "                                                'maxfun':  50000, \n",
    "                                                'maxiter': 100,\n",
    "                                                'iprint': -1,   #print update every 50 iterations\n",
    "                                                'maxls': 1})\n",
    "PINN.set_weights(results.x)\n",
    "v_pred = PINN.evaluate(X_v_test)\n",
    "error = np.linalg.norm((v-v_pred),2)/np.linalg.norm(v,2)\n",
    "error_vec.append(error)\n",
    "iter_vec.append(iteration)\n",
    "\n",
    "\n",
    "while iteration<2000:\n",
    "    iteration += 100\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "    optimizer_L1 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "    optimizer_L2 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "    optimizer_L3 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "    num_epochs = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        loss_value, grads, grads_L1, grads_L2, grads_L3 = PINN.adaptive_gradients()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            tf.print(loss_value)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, PINN.W)) #gradient descent weights \n",
    "        optimizer_L1.apply_gradients(zip([-grads_L1], [PINN.lagrange_1])) # gradient ascent adaptive coefficients of boundary residual\n",
    "        optimizer_L2.apply_gradients(zip([-grads_L2], [PINN.lagrange_2])) # gradient ascent adaptive coefficients of PDE residual\n",
    "        optimizer_L3.apply_gradients(zip([-grads_L3], [PINN.lagrange_3]))  \n",
    "    \n",
    "    results = scipy.optimize.minimize(fun = PINN.optimizerfunc,\n",
    "                                      x0 = results.x,\n",
    "                                      args=(),\n",
    "                                      method='BFGS',\n",
    "                                      jac= True, # If jac is True, fun is assumed to return the gradient along with the objective function\n",
    "                                      callback = None, \n",
    "                                      options = {'disp': None,\n",
    "                                                'maxcor': maxcor, \n",
    "                                                'ftol': 5e-10, # 1 * np.finfo(float).eps,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                                'gtol': 5e-10, \n",
    "                                                'maxfun':  50000, \n",
    "                                                'maxiter': 100,\n",
    "                                                'iprint': -1,   #print update every 50 iterations\n",
    "                                                'maxls': 1})\n",
    "    PINN.set_weights(results.x)\n",
    "    v_pred = PINN.evaluate(X_v_test)\n",
    "    error = np.linalg.norm((v-v_pred),2)/np.linalg.norm(v,2)\n",
    "    error_vec.append(error)\n",
    "    iter_vec.append(iteration)\n",
    "    \n",
    "elapsed = time.time() - start_time              \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "print(iter_vec)\n",
    "print(error_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
